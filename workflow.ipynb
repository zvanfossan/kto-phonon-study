{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decission Tree Regression Workflow\n",
    "\n",
    "This file demonstrates the workflow used for optimizing, training, and evaluating the decission tree regression models developed for predicting the soft phonon frequency of KTaO3 based on irreducible representation order parameters, strain tensor components, and structural metrics.\n",
    "\n",
    "### Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/irrep_order_parameter_magnitudes_and_phonon_frequency_dataset.csv')\n",
    "df1 = pd.read_csv('data/strain_tensor_component_and_phonon_frequency_dataset.csv')\n",
    "df2 = pd.read_csv('data/octahedron_structure_metrics_and_phonon_frequency_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets to be trained on:\n",
    "\n",
    "Irreducible representation order parameter magnitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     gm1  gm3_a0  gm3_0a  gm5_a00  gm5_0a0  gm5_00a    frequency\n",
      "0 -0.005     0.0 -0.0024      0.0    -0.01     0.00  7909.790147\n",
      "1 -0.005     0.0 -0.0024      0.0    -0.01     0.01  7619.874234\n",
      "2 -0.005     0.0 -0.0024      0.0    -0.01    -0.01  7619.800560\n",
      "3 -0.005     0.0 -0.0024      0.0     0.00    -0.01  7620.930328\n",
      "4 -0.005     0.0 -0.0024      0.0     0.00     0.00  7888.340652\n",
      "(729, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strain tensor components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    exx  eyy   ezz   exy   exz   eyz     frequency\n",
      "0  0.01  0.0 -0.01 -0.01 -0.01  0.01 -11388.702497\n",
      "1  0.01  0.0 -0.01 -0.01 -0.01  0.00 -11396.867022\n",
      "2  0.01  0.0 -0.01 -0.01 -0.01 -0.01 -11384.635415\n",
      "3  0.01  0.0 -0.01 -0.01  0.01 -0.01 -11388.595566\n",
      "4  0.01  0.0 -0.01 -0.01  0.01  0.00 -11396.777134\n",
      "(729, 7)\n"
     ]
    }
   ],
   "source": [
    "print(df1.head())\n",
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Octahedron structure metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   average bond length  distortion index  bond angle variance    frequency\n",
      "0             1.988859          0.001129             0.240535  7909.790147\n",
      "1             1.988893          0.001140             0.480232  7619.874234\n",
      "2             1.988893          0.001140             0.480232  7619.800560\n",
      "3             1.988859          0.001146             0.239718  7620.930328\n",
      "4             1.988826          0.001135             0.000000  7888.340652\n",
      "(729, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df2.head())\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each of the above dataframes to their respective target matrices and feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_param_features = df[['gm1','gm3_a0','gm3_0a','gm5_a00','gm5_0a0','gm5_00a']]\n",
    "order_param_target = df['frequency']\n",
    "\n",
    "strain_component_features = df1[['exx','eyy','ezz','exy','exz','eyz']]\n",
    "strain_component_target = df1['frequency']\n",
    "\n",
    "structure_metric_features = df2[['average bond length','distortion index','bond angle variance']]\n",
    "structure_metric_target = df2['frequency']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale each of the datasets into training and testing sets. We chose to use a 75/25 split as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "\n",
    "#Split data into test and train sets\n",
    "irrep_features_train, irrep_features_test, irrep_target_train, irrep_target_test = train_test_split(order_param_features, \n",
    "                                                                                                    order_param_target, \n",
    "                                                                                                    test_size=0.25, random_state=11)\n",
    "strain_features_train, strain_features_test, strain_target_train, strain_target_test = train_test_split(strain_component_features, \n",
    "                                                                                                    strain_component_target, \n",
    "                                                                                                    test_size=0.25, random_state=4)\n",
    "struc_features_train, struc_features_test, struc_target_train, struc_target_test = train_test_split(structure_metric_features, \n",
    "                                                                                                    structure_metric_target, \n",
    "                                                                                                    test_size=0.25, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the training feature datasets. We chose to use the StandardScalar package provided by Scikit Learn since the feature distributions in each dataset are normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "irrep_scaler = preprocessing.StandardScaler().fit(irrep_features_train)  \n",
    "irrep_features_train_scaled = irrep_scaler.transform(irrep_features_train)\n",
    "irrep_features_test_scaled = irrep_scaler.transform(irrep_features_test)\n",
    "\n",
    "strain_scaler = preprocessing.StandardScaler().fit(strain_features_train)  \n",
    "strain_features_train_scaled = strain_scaler.transform(strain_features_train)\n",
    "strain_features_test_scaled = strain_scaler.transform(strain_features_test)\n",
    "\n",
    "struc_scaler = preprocessing.StandardScaler().fit(struc_features_train)  \n",
    "struc_features_train_scaled = struc_scaler.transform(struc_features_train)\n",
    "struc_features_test_scaled = struc_scaler.transform(struc_features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cross-validation to perform a grid search for the most optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare functions for performing cross validation and calculating error\n",
    "def get_rmse(actual, pred):\n",
    "    return np.mean([(actual[i]-pred[i])**2 for i in range(len(actual))])**0.5\n",
    "\n",
    "def run_cv(n_folds, model, X_train, y_train, stratify=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        n_folds (int) : how many folds of CV to do\n",
    "        model (sklearn Model) : what model do we want to fit\n",
    "        X_train (np.array) : feature matrix\n",
    "        y_train (np.array) : target array\n",
    "        stratify (bool) : if True, use stratified CV, otherwise, use random CV\n",
    "        \n",
    "    Returns:\n",
    "        a dictionary with scores from each fold for training and validation\n",
    "            {'train' : [list of training scores],\n",
    "             'val' : [list of validation scores]}\n",
    "            - the length of each list = n_folds\n",
    "    \"\"\"\n",
    "    if stratify:\n",
    "        folds = StratifiedKFold(n_splits=n_folds).split(X_train, y_train)\n",
    "    else:\n",
    "        folds = KFold(n_splits=n_folds).split(X_train, y_train)\n",
    "\n",
    "    train_scores, val_scores = [], []\n",
    "    for k, (train, val) in enumerate(folds):\n",
    "\n",
    "        X_train_cv = X_train[train]\n",
    "        y_train_cv = y_train[train]\n",
    "\n",
    "        X_val_cv = X_train[val]\n",
    "        y_val_cv = y_train[val]\n",
    "\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_train_cv_pred = model.predict(X_train_cv)\n",
    "        y_val_cv_pred = model.predict(X_val_cv)\n",
    "\n",
    "        train_acc = get_rmse(y_train_cv, y_train_cv_pred)\n",
    "        val_acc = get_rmse(y_val_cv, y_val_cv_pred)\n",
    "\n",
    "        train_scores.append(train_acc)\n",
    "        val_scores.append(val_acc)\n",
    "\n",
    "    print('%i Folds' % n_folds)\n",
    "    print('Mean training error = %.3f +/- %.4f' % (np.mean(train_scores), np.std(train_scores)))\n",
    "    print('Mean validation error = %.3f +/- %.4f' % (np.mean(val_scores), np.std(val_scores)))\n",
    "    \n",
    "    training_rmse.append(np.mean(train_scores))\n",
    "    training_std.append(np.std(train_scores))\n",
    "    validation_rmse.append(np.mean(val_scores))\n",
    "    validation_std.append(np.std(val_scores))\n",
    "    \n",
    "\n",
    "    return {'train' : train_scores,\n",
    "           'val' : val_scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to perform a 10-fold cross-validation. The hyperparameters we consider for devloping decission tree regression models are the max depth, minimum samples per leaf, and the mininum weight fraction per leaf. Please refer to the documentation provided by Scikit Learn for greater detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
